---
layout: post
title: PCA via Spectral Decomposition
tags: [pca, spectral decomposition, svd]
---

In this blogpost I am gonna write about computing PCA via spectral (eigenvalue) decomposition of the data covariance matrix. I'll try to put together essential proofs and code which I believe makes understanding easier. Finally, I am gonna talk about some applications in Machine Learning. 

Let's suppose we are given $n$ datapoints, each represented as a vector $x\in\mathbb{R}^d$. We can stack these vectors in a data matrix as follows:
<br /><br />

<center>
$$
X_{n\times{d}}=
\begin{bmatrix}
    \text{---} \hspace{-0.2cm} & x_1^T & \hspace{-0.2cm} \text{---} \\
    \text{---} \hspace{-0.2cm} & x_2^T & \hspace{-0.2cm} \text{---} \\
    \text{   } \hspace{-0.2cm} & \cdots & \hspace{-0.2cm} \text{   } \\
    \text{---} \hspace{-0.2cm} & x_n^T & \hspace{-0.2cm} \text{---} \\
\end{bmatrix}
$$
</center>

<br /><br />
**Definition 1:** For a given random vector $v\in\mathbb{R}^d$, a covariance matrix $K$ is defined as a $d\times{d}$ matrix where 
$K_{ij}=\mathrm{Cov}\[v_i, v_j\]=\mathbb{E}\[(v_i-\mathbb{E}\[v_i\])(v_j-\mathbb{E}\[v_j\])]=\mathbb{E}\[v_i v_j]-\mathbb{E}\[v_i]*\mathbb{E}\[v_j]$ and $v_i$ means $i$-th element of the vector.  

If we assume that our data matrix represents a sample drawn from some underlying distribution, 
we can write an empirical covariance matrix (MLE estimate) as follows: $S_{ij}=\frac{1}{n}\sum_{k=1}^n{(x_{ki}-\bar{x}\_{i})(x_{kj}-\bar{x}\_{j})}$, where $x_{ab}$ means $b$-th element of $a$-th datapoint vector (row of data matrix) and $\bar{x}=\frac{1}{n}\sum_{i=1}^n{x_i}$ is empirical mean vector. Using matrix notation, the same formula can be written as: $S=\frac{1}{n}\sum_{i=1}^n{x_i\cdot{x_i^T}-\bar{x}\cdot{\bar{x}^T}}$. We can go further and remove sum notation, so that our final equation is:<br /><br />
<center>
$$S=\frac{1}{n}X^TX-\frac{1}{n^2}X^T\mathbb{1}_n\mathbb{1}_n^TX\hspace{1cm}(1)$$
</center>
<br />where $\mathbb{1}_n$ is $n$ dimensional vector of ones and $\bar{x}=\frac{1}{n}X^T\mathbb{1}_n$

Note that, if the data is already centered (zero mean across feature dimensions), then empirical data covariance matrix has simpler form:<br /><br />  
<center>$$S=\frac{1}{n}X^TX\hspace{1cm}(2)$$</center>
<br />
(In practice we usually center the data and only after proceed with PCA. Also, we could've used $\frac{1}{n-1}$ instead of $\frac{1}{n}$ to get unbiased estimator, 
however, the latter makes some of math equations cleaner).

Now let's see the math details about how equations (1) and (2) relate to each other.
Note that, if we pull $X$ out of parentheses in the last equation from both sides, 
we get: $\frac{1}{n}X^T(\mathbb{1}\_{n\times{n}}-\frac{1}{n}\mathbb{1}\_n{\mathbb{1}\_n^T})X$, 
where $\mathbb{1}\_{n\times{n}}$ is an identity matrix. 
Let's denote the middle part by $H=\mathbb{1}_{n\times{n}}-\frac{1}{n}\mathbb{1}_n{\mathbb{1}_n^T}$. 
If we look closer, we can see that $H$ is an orthogonal projection matrix on the
 vector orthogonal to $\mathbb{1}_n$. 
 
(Generally, for some vector $v$, $P=\frac{vv^T}{v^Tv}$ is an orthogonal projection matrix on $v$ and $\mathbb{1}_{n\times{n}}-P$ is an orthogonal projection on the vector orthogonal to $v$. For orthogonal projection matrices $P$, we have $P=P^2=P^T$).

$H$ plays a role of centering the data (subtracting mean). Indeed, for vector $v$, 
<center>
$$Hv=(\mathbb{1}_{n\times{n}}-\frac{1}{n}\mathbb{1}_n{\mathbb{1}_n^T})v=
\begin{bmatrix}
    \text{---} \hspace{-0.2cm} & v_1-\bar{v} & \hspace{-0.2cm} \text{---} \\
    \text{---} \hspace{-0.2cm} & v_2-\bar{v} & \hspace{-0.2cm} \text{---} \\
    \text{   } \hspace{-0.2cm} & \cdots & \hspace{-0.2cm} \text{   } \\
    \text{---} \hspace{-0.2cm} & v_n-\bar{v} & \hspace{-0.2cm} \text{---} \\
\end{bmatrix}$$
</center>

and we have $S=\frac{1}{n}X^THX=\frac{1}{n}X^THHX=\frac{1}{n}(HX)^T(HX)$, where $HX$ is centered data matrix.

Let's summarize some facts about the matrix $S$:  
 - It's **symmetric** ($S^T=\frac{1}{n}(X^TX)^T=S$)
 - It's **positive semi-definite** ($x^T(\frac{1}{n}X^TX)x=\frac{1}{n}(Xx)^T(Xx)=\frac{1}{n}\lVert Xx \rVert \geq 0$)
 - Since $X^TX$ and $X$ have the same nullspace (see [proof](https://math.stackexchange.com/questions/945259/proving-that-the-nullspacea-nullspaceata-where-a-is-a-real-m-x-n-ma)), if the data matrix $X$ has a full column rank (which is usually the case in practice), $S$ is **positive-definite** and, therefore, has **positive eigenvalues**.
 
 So far, we've shown how to compute MLE estimate of covariance matrix from the given data. Now let's show how to use it for computing PCA.

**Proposition 1:** For any unit vector $u\in\mathbb{R}^d$, the variance of the data along the direction $u$ can be computed as $u^T\Sigma{u}$, where $\Sigma$ is a $d\times{d}$ data covariance matrix. By saying variance along the direction, we mean the variance of the projected points on that particular direction.  <br />  
**Proof:**  Let $x\in\mathbb{R}^d$ be some random vector with covariance matrix $\Sigma$. $u^T\Sigma{u}=u^T(\mathbb{E}\[xx^T]-\mathbb{E}\[x]\mathbb{E}\[x^T])u=
\mathbb{E}\[(u^Tx)(x^Tu)]-\mathbb{E}\[(u^Tx)]\mathbb{E}\[(u^Tx)]=\mathbb{E}\[(u^Tx)^2]-
\mathbb{E}\[u^Tx]^2=\mathrm{Var}(u^Tx)$. Indeed, geometrically, $u^Tx$ is the distance from the origin to the orthogonal projection point of $x$ on $u$ (also note that, the covariance 
matrix being always at least positive semi-definite $u^T\Sigma{u}\geq{0}$ aligns with the fact that variance is always non-negative).<br /><br />

Since our empirical covariance matrix $S$ is symmetric and positive-definite (we assume full column rank of the data), we can perform a spectral (eigenvalue) decomposition of it and get $S_{d\times{d}}=P_{d\times{d}}D_{d\times{d}}P_{d\times{d}}^T$, where $P$ is an orthonormal matrix having $S$'s orthonormal eigenvectors as its columns. Meanwhile, $D$ is a diagonal matrix, containing corresponding eigenvalues on diagonal (since $S$ is real symmetric, all eigenvalues are real numbers as well). We can always arrange columns in a way that eigenvalues come in non-increasing order from top to bottom across the diagonal: 

<center>
$$S=
\begin{bmatrix}
    \vert & \vert \\
    v_1   & v_d   \\
    \vert & \vert
\end{bmatrix}
\begin{bmatrix}
    \lambda_{1} & & \\
    & \ddots & \\
    & & \lambda_{d}
  \end{bmatrix}
\begin{bmatrix}
    \text{---} & v_1 & \text{---} \\
    \text{---} & v_d & \text{---}
\end{bmatrix}, \lambda_1 \geq \lambda_1  \geq \cdots \geq \lambda_d \in \mathbb{R}$$</center><br /><br />

**Proposition 2:** The direction of the maximum variance of the data and the variance itself are given by first (corresponding to the largest eigenvalue) eigenvector and eigenvalue, respectively. 
$\max_{\lVert u \rVert = 1}u^TSu=\lambda_1$ and $arg\max_{\lVert u \rVert = 1}u^TSu=v_1$. I.e. the maximal data spread is across eigenvector corresponding to the largest eigenvalue.

**Proof:** $u^T\Sigma{u}=u^TPDP^Tu=(P^Tu)^TD(P^Tu)=\sum_{i=1}^dD_{ii}(P^Tu)\_i^2\leq \sum_{i=1}^dD_{11}(P^Tu)\_i^2=
\lambda_1\sum_{i=1}^d(P^Tu)_i^2=\lambda_1\lVert P^Tu \rVert _2 ^ 2=\lambda_1$. Observe that, since $P$ is orthonormal and $u$ is unit vector, L2 norm equals to 1 for any $u$.  

Equality holds when we set $u=v_1$, since dot products $(P^Tv_1)\_i^2$, $i>1$ will disappear due to orthogonality. Thus, we get $\sum_{i=1}^dD_{ii}(P^Tv_1)_i^2=\lambda_1$ 

**Proposition 3:**  $\max_{\lVert u \rVert = 1, u \perp v_1}u^TSu=\lambda_2$ and $arg\max_{\lVert u \rVert = 1, u \perp v_1}u^TSu=v_2$. I.e. the maximal data spread and the direction in the $v_1$'s orthogonal complement space are given by the second-largest eigenvalue and eigenvector, respectively. The same statement applies to the third, four and the rest of eigenvectors and eigenvalues.<br />

**Proof:** Similar to above proof, if $u \perp v_1$, then $(P^Tu)\_1=0$ and $u^T\Sigma{u}=u^TPDP^Tu=(P^Tu)^TD(P^Tu)=\sum_{i=2}^dD_{ii}(P^Tu)\_i^2\leq \sum_{i=2}^dD_{22}(P^Tu)\_i^2=
\lambda_2\sum_{i=2}^d(P^Tu)_i^2=\lambda_2\lVert P^Tu \rVert _2 ^ 2=\lambda_2$. 

(Proofs for proposition 2 and 3 can be also derived using constrained optimization with Lagrange multipliers.)

Direction of the largest data spread given by the first eigenvector is referred as **first Principal Axes**, the second largest spread (from the orthogonal complement space) given by the second eigenvector - **second Principal Axes** and so on...

As we already mentioned, eigenvalues give data variance across principal axes. Consequently, the **total variance** of the data is given by $tr(S)=\sum_{i=1}^d\lambda_i$. **Explained variance** of the $i$-th principal axes is defined as $\frac{\lambda_i}{tr(S)}$. For instance, if we project our data on the first principal axes, we will capture $\frac{\lambda_1}{tr(S)}$ fraction of the total data variance, which is maximum we can do when projecting on 1D space.

Below Python code snippet generates random 2D datapoints, computes empirical covariance along with its eigenvalues and eigenvectors. Finally, it plots principal axes of the data.

{% highlight python linenos %}
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

n = 500 # number of datapoints.

# generating data matrix of n datapoints, each 2 dimensional.

rotate, _ = np.linalg.qr(np.random.rand(2, 2)) # obtaining random rotation matrix via QR decomposition.
x = rotate @ np.random.normal(0, 5 * np.random.rand(2), (n, 2)).T
x = x.T

# making sure data is centered.
x -= x.mean(axis=0)

S = (1/n) * x.T @ x # computing empirical covariance matrix (equation 2).

eigvals, eigvecs = np.linalg.eigh(S) # computing eigenvalues and orthonormal eigenvectors.

# making sure that eigenvalues and eigenvectors come in non-decreasing order.
if eigvals[0] < eigvals[1]:
    eigvals = eigvals[[1, 0]]
    eigvecs = eigvecs[:,[1, 0]]

λ1, λ2 = eigvals

# computing principal axes for visualization purposes. 
# (largest and second largest orthonormal eigenvectors scaled by explained variance times some constant)
v1 = 3 * λ1 / (λ1 + λ2) * eigvecs[:,0] 
v2 = 3 * λ2 / (λ1 + λ2) * eigvecs[:,1]

fig, ax = plt.subplots(1, 2, figsize=(12,4))

ax[0].axis('equal')
ax[0].scatter(x[:,0], x[:,1], s=7, c='orange') # plotting the data

# plotting the principal axes
ax[0].quiver([0], 
             [0], 
             [v1[0], v2[0]], 
             [v1[1], v2[1]],
             color=['r', 'b'], 
             scale=10)

ax[0].title.set_text('Principal Axes of the Data')
ax[0].grid()

# rotating data so that standard basis align with eigenbasis (principal axes).
z = x @ eigvecs 

# rotating principal axes to match standard axes (changing basis).
v11 = 3 * λ1 / (λ1 + λ2) * eigvecs.T @ eigvecs[:,0]
v22 = 3 * λ2 / (λ1 + λ2) * eigvecs.T @ eigvecs[:,1]

ax[1].axis('equal')
ax[1].scatter(z[:,0],z[:,1], s=7, c='orange')

# plotting rotated data.
ax[1].quiver([0], 
             [0], 
             [v11[0], v22[0]], 
             [v11[1], v22[1]],
             color=['r', 'b'], 
             scale=10)

ax[1].title.set_text('Rotated Data (Standard Axes align with the Principal Axes)')
ax[1].grid()
{% endhighlight %}

<img src="/img/pca/plot.png" />

In practice, however, PCA is performed by computing **Singular Value Decomposition (SVD)** of data matrix rather than Eigendecomposition of data covariance matrix. The reason usually is that the former is much more numerically stable than the latter. Personally, I find the latter method easier to understand for beginners, it's good to know both, though.

Let's very briefly talk about SVD and show how it related to eigendecomposition we described above. Well, SVD decomposes data matrix as follows: $X_{n\times d}=U_{n\times n}\Sigma_{n\times d}V_{d\times d}^T$, where $V$ and $\Sigma$ are particularly interesting to us, since $V$ contains orthonormal eigenvectors of the empirical data covariance matrix $S$ and $\Sigma$ is diagonal matrix containing square roots from eigenvalues of $S$, called singular values (well, in literature it's said that $V$ contains eigenvectors of $X^TX$ and $S=\frac{1}{n}X^TX$, but, note that, scalar multiplication doesn't affect eigenvalues and eigenvectors).

Finally, let's mention some applications of the PCA:
1. The primary application is **dimensionality reduction**. For instance, if we have 1000 dimensional vectors (rank of data matrix is 1000) and we want to make them smaller, say 10 dimensional, we can project them on the first 10 principal axes and drop the rest. This will also be the best possible low rank (rank=10) approximation to the original data matrix in terms of operator 2 norm or Frobenius norm $\lVert X-Proj(X) \rVert$ (Eckart-Young-Mirsky theorem, see [here](https://stats.stackexchange.com/questions/130721/what-norm-of-the-reconstruction-error-is-minimized-by-the-low-rank-approximation)).
2. In Natural Language Processing, performing PCA on word-to-word co-occurrence matrices results in low dimensional word embeddings similar to word2vec. Similar methods applied to term-document co-occurrence matrix is known as [Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis#:~:text=Latent%20semantic%20analysis%20(LSA)%20is,to%20the%20documents%20and%20terms).   
3. PCA can be used to perform **Total Least Squares** regression (a.k.a orthogonal regression) which optimizes squared distances from points to their orthogonal projection points to the fitted line, as opposed to, **Ordinary Least Squares** regression that minimizes sum of squared residuals. in $N$ dimensional TLS regression problem, the solution would be $N-1$ dimensional hyperplane defined by $N-1$ principal axes.
4. Applying PCA projection on the data matrix also results in **feature decorrelation** (covariance matrix of transformed data is diagonal) which is sometimes desired pre-processing step for some Machine Learning pipelines.
5. PCA is also used for computing Eigenfaces - legacy Computer Vision algorithm for face recognition task.